{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and models saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warning\n",
    "warnings.filterwarnings('ignore', message=\"The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\")\n",
    "\n",
    "# Load data\n",
    "students_df = pd.read_csv('student_data.csv')\n",
    "catalog_df = pd.read_csv('course_catalog.csv')\n",
    "\n",
    "# Define a tokenizer function\n",
    "def custom_tokenizer(text):\n",
    "    return text.split(', ')\n",
    "\n",
    "# Vectorization of subjects and interests\n",
    "vectorizer = CountVectorizer(tokenizer=custom_tokenizer)\n",
    "\n",
    "# Create feature vectors\n",
    "student_vectors = vectorizer.fit_transform(students_df['subjects'] + ', ' + students_df['interests'])\n",
    "course_vectors = vectorizer.transform(catalog_df['subjects'] + ', ' + catalog_df['skills'])\n",
    "\n",
    "# Save the vectorizer to a file\n",
    "joblib.dump(vectorizer, 'vectorizer.joblib')\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_matrix = cosine_similarity(student_vectors, course_vectors)\n",
    "\n",
    "# Generate Recommend courses for each student\n",
    "recommendations = {}\n",
    "for i, student_id in enumerate(students_df['student_id']):\n",
    "    similar_courses = similarity_matrix[i].argsort()[::-1]  # Sort courses by similarity\n",
    "    recommended_courses = catalog_df.iloc[similar_courses[:5]]['course_name'].tolist()  # Top 5 courses\n",
    "    recommendations[student_id] = recommended_courses\n",
    "\n",
    "# Save the similarity matrix (model) to a file\n",
    "joblib.dump(similarity_matrix, 'similarity_matrix.joblib')\n",
    "\n",
    "# Save the recommendations to a CSV file\n",
    "recommendations_df = pd.DataFrame.from_dict(recommendations, orient='index')\n",
    "recommendations_df.to_csv('recommendations.csv')\n",
    "\n",
    "print(\"Training completed and models saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Student 101 is recommended the following courses: Bachelor of Arts in Journalism, Bachelor of Arts in Journalism, Bachelor of Arts in Literature, Bachelor of Arts in Journalism, Bachelor of Arts in Literature\n",
      "New Student 102 is recommended the following courses: Bachelor of Arts in Urban Planning, Bachelor of Arts in Urban Planning, Bachelor of Arts in Urban Planning, Bachelor of Arts in International Relations, Bachelor of Arts in Urban Planning\n"
     ]
    }
   ],
   "source": [
    "# Sample new student data\n",
    "new_students = pd.DataFrame({\n",
    "    'student_id': [101, 102],\n",
    "    'subjects': ['Math, English, Physics', 'History ,Kiswahili, Geography'],\n",
    "    'interests': ['Astronomy, Quantum Mechanics', 'Politics, Cultural Studies']\n",
    "})\n",
    "\n",
    "# Vectorize the new student's data (subjects + interests)\n",
    "new_student_vectors = vectorizer.transform(new_students['subjects'] + ', ' + new_students['interests'])\n",
    "\n",
    "# Calculate cosine similarity between new students and courses\n",
    "new_similarity_matrix = cosine_similarity(new_student_vectors, course_vectors)\n",
    "\n",
    "# Generate recommendations for new students\n",
    "new_recommendations = {}\n",
    "for i, student_id in enumerate(new_students['student_id']):\n",
    "    similar_courses = new_similarity_matrix[i].argsort()[::-1]  # Sort courses by similarity\n",
    "    recommended_courses = catalog_df.iloc[similar_courses[:5]]['course_name'].tolist()  # Top 5 courses\n",
    "    new_recommendations[student_id] = recommended_courses\n",
    "\n",
    "# Display recommendations for the new students\n",
    "for student_id, courses in new_recommendations.items():\n",
    "    print(f\"New Student {student_id} is recommended the following courses: {', '.join(courses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save vectorizer and course vectors to disk\n",
    "with open('vectorizer.pkl', 'wb') as vec_file:\n",
    "    pickle.dump(vectorizer, vec_file)\n",
    "\n",
    "with open('course_vectors.pkl', 'wb') as course_file:\n",
    "    pickle.dump(course_vectors, course_file)\n",
    "\n",
    "# Later, load the vectorizer and course vectors to make new predictions\n",
    "with open('vectorizer.pkl', 'rb') as vec_file:\n",
    "    loaded_vectorizer = pickle.load(vec_file)\n",
    "\n",
    "with open('course_vectors.pkl', 'rb') as course_file:\n",
    "    loaded_course_vectors = pickle.load(course_file)\n",
    "\n",
    "# Test the loaded model with new student data\n",
    "new_student_vectors = loaded_vectorizer.transform(new_students['subjects'] + ', ' + new_students['interests'])\n",
    "new_similarity_matrix = cosine_similarity(new_student_vectors, loaded_course_vectors)\n",
    "\n",
    "# Make predictions for the new students (same as above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "\n",
    "# supress specific warning\n",
    "warnings.filterwarnings('ignore', message=\"The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\")\n",
    "\n",
    "\n",
    "# Load data\n",
    "students_df = pd.read_csv('student_data.csv')\n",
    "catalog_df = pd.read_csv('course_catalog.csv')\n",
    "\n",
    "# Vectorization of subjects and interests\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(', '))\n",
    "\n",
    "# Create feature vectors\n",
    "student_vectors = vectorizer.fit_transform(students_df['subjects'] + ', ' + students_df['interests'])\n",
    "course_vectors = vectorizer.transform(catalog_df['subjects'] + ', ' + catalog_df['skills'])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_matrix = cosine_similarity(student_vectors, course_vectors)\n",
    "\n",
    "\n",
    "# Generate Recommend courses for each student\n",
    "recommendations = {}\n",
    "for i, student_id in enumerate(students_df['student_id']):\n",
    "    similar_courses = similarity_matrix[i].argsort()[::-1]  # Sort courses by similarity\n",
    "    recommended_courses = catalog_df.iloc[similar_courses[:5]]['course_name'].tolist()  # Top 5 courses\n",
    "    recommendations[student_id] = recommended_courses\n",
    "\n",
    "# Display recommendations\n",
    "for student_id, courses in recommendations.items():\n",
    "    print(f\"Student {student_id} is recommended the following courses: {', '.join(courses)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Data: Separate your data into training and testing sets.\n",
    "\n",
    "Train the Model: Create the recommendation model using the training set.\n",
    "\n",
    "Save the Model: Save the model using a library like joblib or pickle.\n",
    "\n",
    "Load the Model: Load the saved model for making predictions.\n",
    "\n",
    "Make Predictions: Use the loaded model to generate recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'S522'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m loaded_vectorizer, loaded_recommendations \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcourse_recommendation_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Now let's predict for the test set\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m similarity_matrix_test \u001b[38;5;241m=\u001b[39m cosine_similarity(X_test, course_vectors)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Generate recommendations for the test set\u001b[39;00m\n\u001b[0;32m     45\u001b[0m recommendations_test \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Tiff\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Tiff\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1657\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1614\u001b[0m \n\u001b[0;32m   1615\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1657\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[0;32m   1659\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\Tiff\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:164\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    155\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    156\u001b[0m         X,\n\u001b[0;32m    157\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    165\u001b[0m         X,\n\u001b[0;32m    166\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m    167\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    168\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    169\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m    170\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    171\u001b[0m     )\n\u001b[0;32m    172\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    173\u001b[0m         Y,\n\u001b[0;32m    174\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    179\u001b[0m     )\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n",
      "File \u001b[1;32mc:\\Users\\Tiff\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tiff\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'S522'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Define a tokenizer function\n",
    "def custom_tokenizer(text):\n",
    "    return text.split(', ')\n",
    "\n",
    "# Load data\n",
    "students_df = pd.read_csv('student_data.csv')\n",
    "catalog_df = pd.read_csv('course_catalog.csv')\n",
    "\n",
    "# Vectorization of subjects and interests\n",
    "vectorizer = CountVectorizer(tokenizer=custom_tokenizer)\n",
    "\n",
    "# Create feature vectors\n",
    "student_vectors = vectorizer.fit_transform(students_df['subjects'] + ', ' + students_df['interests'])\n",
    "course_vectors = vectorizer.transform(catalog_df['subjects'] + ', ' + catalog_df['skills'])\n",
    "\n",
    "# Split the student data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(students_df, student_vectors, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate cosine similarity for the training set\n",
    "similarity_matrix_train = cosine_similarity(y_train, course_vectors)\n",
    "\n",
    "# Generate recommendations for the training set\n",
    "recommendations_train = {}\n",
    "for i, student_id in enumerate(X_train['student_id']):  # Use only training set\n",
    "    similar_courses = similarity_matrix_train[i].argsort()[::-1]  # Sort courses by similarity\n",
    "    recommended_courses = catalog_df.iloc[similar_courses[:5]]['course_name'].tolist()  # Top 5 courses\n",
    "    recommendations_train[student_id] = recommended_courses\n",
    "\n",
    "# Save the vectorizer and recommendations model\n",
    "joblib.dump((vectorizer, recommendations_train), 'course_recommendation_model.pkl')\n",
    "\n",
    "# Load the model\n",
    "loaded_vectorizer, loaded_recommendations = joblib.load('course_recommendation_model.pkl')\n",
    "\n",
    "# Now let's predict for the test set\n",
    "similarity_matrix_test = cosine_similarity(X_test, course_vectors)\n",
    "\n",
    "# Generate recommendations for the test set\n",
    "recommendations_test = {}\n",
    "for i, student_id in enumerate(X_test['student_id']):  # Use only test set\n",
    "    similar_courses = similarity_matrix_test[i].argsort()[::-1]  # Sort courses by similarity\n",
    "    recommended_courses = catalog_df.iloc[similar_courses[:5]]['course_name'].tolist()  # Top 5 courses\n",
    "    recommendations_test[student_id] = recommended_courses\n",
    "\n",
    "# Display test recommendations\n",
    "for student_id, courses in recommendations_test.items():\n",
    "    print(f\"Student {student_id} is recommended the following courses: {', '.join(courses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('student_data.csv')\n",
    "\n",
    "# Step 2: Drop duplicate rows based on all columns\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Step 3: Save the cleaned DataFrame back to a CSV file\n",
    "df_cleaned.to_csv('student_data_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
